# Experiment Title: MDP with Preferences (Quantitative) 

This experiment implements code from Automatica 2025 submission w/ Hazhar, Jie Fu.

1. ✅ Implement MDP model.
2. ✅ Define preference specifications and translate to automaton. 
3. ✅ Compute product of MDP and preference DFA. 
4. ✅ Flatten the symbolically defined product. 
5. ✅ Assign reward to the flattened product states.
6. ✅ Compute the optimal policy for the flattened product 
    > How? I don't understand Hazhar's implementation. Can we invoke existing MDP solvers?
7. ✅ Define simulator for bee-robot environment.
8. ✅ Run synthesized policy to visualize behavior.
9. ✅ Define gym environment for bee-robot environment. 
10. ✅ Run synthesized policy to visualize behavior.

### TODO: Refactoring
* Move `StochasticOrderType, ProductGame, PrefGraphGame, Solver` classes to `ggsolver` package.
* Move `stochastic_weak_order, stochastic_weak_star_order, stochastic_strong_order` to `ggsolver` package.
* Move `spot_eval` and PrefAutomaton patch to `ggsolver` package.

### TODO: Experiment - Scalability
This experiment should show 
1. How to synthesize a single strategy in this domain. 
2. The outcome of the chosen strategy. 
3. The time taken to solve MDPs for various size parameters.

Like Meilun's paper, let's consider synthesizing with three preference goals.
The timings should be reported for all three stochastic orders.

* Implement Robot `Navigation` example (adopt from RDDLSim - Meilun Li's paper).
* Implement Robot `Reconnaissance` example (adopt from RDDLSim - Meilun Li's paper).
* Implement Robot `Crossing Traffic` example (adopt from RDDLSim - Meilun Li's paper).

Also remark that under a fixed weight, the problem reduces to complete preference.
Hence, Meilun's solvers will also work. 
However, due to the stochastic ordering idea, the solution is guaranteed to be Pareto-optimal. 


### TODO: Experiment - Ordering comparison
This experiment should show how outcome probabilities vary when equal 
weights are assigned to each ordering class.

* Use `Navigation/CrossTraffic/Reconnaissance` example. 
* Run three orders with equal weights.
* Plot bar-chart showing probability of satisfying each objective under three orders.

### TODO: Experiment - Effect of stochasticity
* Plot the same for increased stochasticity case (see current paper/effect of stochasticity).

### TODO: Experiment - Effect of weights
* Pareto plot from current paper.


> **MDP Solver Interfaces [FAILURE].**
> 
> For solving the game, it is desirable to use existing optimized solvers.   
> Three options were explored: PRISM, stormpy, MDPToolbox.
> 
> **PyMDPToolbox**: The tool is not suitable for large MDPs.
> Specifically, the tool requires `np.array` with `float64` precision. 
> This demands a lot of RAM, and process is killed by OS. 
> Moreover, the tool checks if matrix is stochastic. 
> This process is inefficiently implemented, and takes a lot of time.   
> 
> **PRISM:** I simply couldn't import the model generated by ggsolver into PRISM
> and synthesize a strategy. 
> 
> **stormpy:** Doesn't support MDP value iteration when Pr(Reach(F)) < 1.0.
> Hence, this tool is not suitable for our purpose.
