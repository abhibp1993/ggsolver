# Experiment Title: MDP with Preferences (Quantitative) 

This experiment implements code from Automatica 2025 submission w/ Hazhar, Jie Fu.

1. ✅ Implement MDP model.
2. ✅ Define preference specifications and translate to automaton. 
3. ✅ Compute product of MDP and preference DFA. 
4. ✅ Flatten the symbolically defined product. 
5. ✅ Assign reward to the flattened product states.
6. ✅ Compute the optimal policy for the flattened product 
    > How? I don't understand Hazhar's implementation. Can we invoke existing MDP solvers?
7. ✅ Define simulator for bee-robot environment.
8. ✅ Run synthesized policy to visualize behavior.
9. ✅ Define gym environment for bee-robot environment. 
10. ✅ Run synthesized policy to visualize behavior.


> **MDP Solver Interfaces [FAILURE].**
> 
> For solving the game, it is desirable to use existing optimized solvers.   
> Three options were explored: PRISM, stormpy, MDPToolbox.
> 
> **PyMDPToolbox**: The tool is not suitable for large MDPs.
> Specifically, the tool requires `np.array` with `float64` precision. 
> This demands a lot of RAM, and process is killed by OS. 
> Moreover, the tool checks if matrix is stochastic. 
> This process is inefficiently implemented, and takes a lot of time.   
> 
> **PRISM:** I simply couldn't import the model generated by ggsolver into PRISM
> and synthesize a strategy. 
> 
> **stormpy:** Doesn't support MDP value iteration when Pr(Reach(F)) < 1.0.
> Hence, this tool is not suitable for our purpose.
